# -*- coding: utf-8 -*-
"""LD1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KXwrva0mjDNFZTvizBTbNzcvPgtYI1v
"""

# !pip install transformers datasets

import time
import torch
from pathlib import Path
from sklearn.metrics import accuracy_score
import datetime
import random
import csv
from datasets import load_dataset, load_metric
from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer

model_name = 'gpt2'

device = "cuda:0" if torch.cuda.is_available() else "cpu"
fix_seed = 42
torch.manual_seed(fix_seed)

# mix train set
train_ds = {
    # 'train': "./data/bestFriend_and_abortion._v1_train.csv",
    # 'dev': "./data/bestFriend_and_abortion._v1_train.csv"

    # 'train': "./data/bestFriend._v1_train.csv",
    # 'dev': "./data/bestFriend._v1_train.csv"

    # 'train': "./data/abortion._v1_train.csv",
    # 'dev': "./data/abortion._v1_train.csv"

    'train': "./data/mix_train_hotels_reviews_v2.csv",
    'dev': "./data/mix_train_hotels_reviews_v2.csv"

    # 'train': "./data/fr_train5555_25.csv",
    # 'train': "./data/fr_train5555.csv",
    # 'dev': "./data/fr_test5555.csv",

    # 'train': "./data/hotels_train_v4.csv",
    # 'dev': "./data/hotels_train_v4.csv"

    # 'train': "./data/hotel-deceptive-opinion3-train.csv",
    # 'dev': "./data/hotel-deceptive-opinion3-dev.csv"
}

other_ds = {
    'politic_v1': "./data/politic-v2.csv",
    #'hotels_dev': "./data/hotel-deceptive-opinion3-dev.csv",
    'hotels_dev': "./data/hotels_dev_v4.csv",
    'reviews_dev': "./data/fr_test5555.csv",
    'deathPenalty_dev': "./data/deathPenalty._v1_test.csv",
    'bestFriend_dev': "./data/bestFriend._v1_test.csv",
    'abortion_dev': "./data/abortion._v1_test.csv"    
}

raw_train = load_dataset("csv", data_files=train_ds)
raw_other = load_dataset("csv", data_files=other_ds)

raw_train['train'][0]

raw_train['dev'][1]

raw_other['politic_v1'][1]

raw_other['hotels_dev'][3]

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # this is what made the different!

tokenized_ds = raw_train.map(tokenizer, input_columns='text', fn_kwargs={
        "max_length": 128, "truncation": True, "padding": "max_length"})
tokenized_ds.set_format('torch')
for split in tokenized_ds:
    tokenized_ds[split] = tokenized_ds[split].add_column('label', raw_train[split]['label'])

tokenized_other_ds = raw_other.map(tokenizer, input_columns='text', fn_kwargs={
        "max_length": 128, "truncation": True, "padding": "max_length"})
tokenized_other_ds.set_format('torch')
for split in tokenized_other_ds:
    tokenized_other_ds[split] = tokenized_other_ds[split].add_column('label', raw_other[split]['label'])

def metric_fn(predictions):
  preds = predictions.predictions.argmax(axis=1)
  labels = predictions.label_ids
  curr_acc = accuracy_score(preds, labels)
  return {'accuracy': curr_acc,
          'eval_accuracy': curr_acc,
          'eval_dev_accuracy': curr_acc}

OUT_PATH = Path("./data")

def build_and_evaluate_model(num_train_epochs=4, seed=42, per_device_train_batch_size=10, per_device_eval_batch_size=128,
                             learning_rate=1e-05, weight_decay=0, adam_beta1=0.9, warmup_ratio=0, adafactor=False,
                             skip_bad_results: float = 0, save_score=True):
  model_clf = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)
  # model_clf = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
  model_clf.config.pad_token_id = model_clf.config.eos_token_id

  torch.manual_seed(seed)
  # args = TrainingArguments(output_dir=OUT_PATH, load_best_model_at_end=False,
  #                          overwrite_output_dir=True, per_device_train_batch_size=per_device_train_batch_size,
  #                          per_device_eval_batch_size=per_device_eval_batch_size, evaluation_strategy='epoch',
  #                          metric_for_best_model='dev_accuracy', greater_is_better=True, do_train=True,
  #                          num_train_epochs=num_train_epochs, report_to='none', seed=seed, weight_decay=weight_decay,
  #                          learning_rate=learning_rate, adam_beta1=adam_beta1, warmup_ratio=warmup_ratio,
  #                          adafactor=adafactor)

  args = TrainingArguments(output_dir=OUT_PATH, load_best_model_at_end=False,
                             overwrite_output_dir=True, per_device_train_batch_size=per_device_train_batch_size,
                             per_device_eval_batch_size=per_device_eval_batch_size, evaluation_strategy='epoch',
                             metric_for_best_model='dev_accuracy', greater_is_better=True, do_train=True,
                             num_train_epochs=num_train_epochs, report_to='none', seed=seed)
  

  # args = TrainingArguments(output_dir=OUT_PATH, load_best_model_at_end=False,
  #                          overwrite_output_dir=True, per_device_train_batch_size=per_device_train_batch_size,
  #                          per_device_eval_batch_size=per_device_eval_batch_size, evaluation_strategy='epoch',
  #                          metric_for_best_model='dev_accuracy', greater_is_better=True, do_train=True,
  #                          num_train_epochs=num_train_epochs, report_to='none', seed=seed, learning_rate=learning_rate)

  trainer = Trainer(
      model=model_clf,
      args=args,
      train_dataset=tokenized_ds['train'],
      eval_dataset=tokenized_ds['train'],
      # eval_dataset=tokenized_ds['dev'],
      compute_metrics=metric_fn
  )

  trainer.train()
  # tmp_name = f"{model_name}_tmp_name"
  # model_clf.save_pretrained(str(SAVED_TO_PATH / tmp_name))

  res_dev = trainer.predict(tokenized_ds['dev'])
  print(f'\nAccuracy score on dev is = {res_dev.metrics["test_accuracy"]}')

  return trainer

trainer1 = build_and_evaluate_model()

res_dev = trainer1.predict(tokenized_ds['dev'])
print(f'\n>>> Accuracy score on dev is = {res_dev.metrics["test_accuracy"]} >>> \n')

res_politics = trainer1.predict(tokenized_other_ds['politic_v1'])
print(f'\n>>> Accuracy score on politic-v1 is = {res_politics.metrics["test_accuracy"]} >>> \n')

res_deathPenalty = trainer1.predict(tokenized_other_ds['deathPenalty_dev'])
print(f'\n>>> Accuracy score on deathPenalty is = {res_deathPenalty.metrics["test_accuracy"]} >>> \n')
# when train on bestFriend -> 0.47 accuracy

res_bestFriend = trainer1.predict(tokenized_other_ds['bestFriend_dev'])
print(f'\n>>> Accuracy score on bestFriend is = {res_bestFriend.metrics["test_accuracy"]} >>> \n')
# when train on bestFriend -> 0.71 accuracy

res_abortion = trainer1.predict(tokenized_other_ds['abortion_dev'])
print(f'\n>>> Accuracy score on abortion is = {res_abortion.metrics["test_accuracy"]} >>> \n')
# when train on bestFriend -> 0.5454 accuracy

res_hotels = trainer1.predict(tokenized_other_ds['hotels_dev'])
print(f'\n>>> Accuracy score on hotels is = {res_hotels.metrics["test_accuracy"]} >>> \n')
# when train on mix (review+hotels) -> ~0.85 accuracy
# hotels on hotels -? 0.85+
# when train with reviews -> around 0.58
# when train on bestFriend -> 0.49 accuracy

res_reviews = trainer1.predict(tokenized_other_ds['reviews_dev'])
print(f'\n>>> Accuracy score on reviews is = {res_reviews.metrics["test_accuracy"]} >>> \n')
# when train on mix (review+hotels) -> ~0.9 accuracy
# when train only on hotel -> gets 0.58 accuracy
# when train on rviews -> 0.9+ accuracy